{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes Classifier- spam & not spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing data\n",
    "spamtr=[open('spam-train/spamtrain{}.txt'.format(i)).read() for i in range(349)] #350 file\n",
    "nospamtr=[open('nonspam-train/nonspamtrain{}.txt'.format(i)).read() for i in range(349)]\n",
    "spamtest=[open('spam-test/spam{}.txt'.format(i)).read() for i in range(129)]\n",
    "nospamtest=[open('nonspam-test/nonspam{}.txt'.format(i)).read() for i in range(129)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'great parttime summer job display box credit application need place small owneroperate store area here introduce yourself store owner manager our effective script tell little display box save customer hundred dollar draw card business every app send spot counter place box nothing need need name address company send commission check compensaation every box place become representative earn commission each application store course much profitable plan pay month small effort call code hours receive detail removed our mailing list type b hotmail com area remove subject area e mail send '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The data is now stored in 4 lists. See this for example\n",
    "spamtr[0]\n",
    "#nospamtr[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we should tokenize these into a meaningful unit, such as words. There are several method for tokenizing words. For example, we can use word_tokenize from nltk. Or we can use find() function. Note that find() is better here becasue it handles punctuation as well. see appendix (end of the notebook) for examples.\n",
    "\n",
    "We want to know in how many documents each word has occured (not how many times total or how many times per document). So we iterate over documents and we use \"set\" to count multiple occurances in each document only once. (see appendix for details)\n",
    "\n",
    "We will define two counters. One for words in spam documents and one for words in non spam documents. We also use np.sum instead of regular sum becasue regular sum doesn't support sum for counter objects. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sp = np.sum([Counter(set(re.findall(\"[a-z0-9']+\",txt.lower()))) for txt in spamtr])\n",
    "n_nonsp = np.sum([Counter(set(re.findall(\"[a-z0-9']+\",txt.lower()))) for txt in nospamtr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#see what n_sp is (a dictionary with counts)\n",
    "#n_sp \n",
    "#n_sp[\"month\"] # we can get the count this way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see most common words using most_common() function from counter. In this data set seems like words such as \"a, the, an\" have already been removed. If it wasn't the case, we'd need to ignore top 10-20 words or all those words that occur in all documents becasue they don't provide any useful information for classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_sp.most_common()[:10] #top 10 most common words in all spam emails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at Bayes formula:\n",
    "\n",
    "$$\n",
    "p({\\rm spam}\\ |\\ words) = \\frac{p(words\\ |\\ {\\rm spam})\\,p({\\rm spam})}\n",
    "{p(words\\ |\\ {\\rm spam})\\,p({\\rm spam})+p(words\\ |\\ {\\rm nonSpam})\\,p({\\rm nonSpam})}\n",
    "\\approx\\frac{\\prod_i p(w_i\\ |\\ {\\rm spam})}{\\prod_i p(w_i\\ |\\ {\\rm spam})+\\prod_i p(w_i\\ |\\ {\\rm nonSpam})}\n",
    "$$\n",
    "\n",
    "Note: For simplicity, we assume p(spam)=P(not spam)= 0.5 here.\n",
    "(The second line says we multiply this for each word)\n",
    "\n",
    "What does the formula say? \n",
    "It says that for each word in the \"unknown\"/test document, first we take a look at the probability of that word in both train sets (spam train set and non spam train set). For example, if the word \"award\" occured in 200 documents out of 350 spam trainset then P(\"award\"|spam)=200/350. Similarly we get a P(\"award\"|nonSpam)).\n",
    "The unknown document has several words and we want to look at all the common words in it. It's a choice on how many common words we keep but adding too many could add noise. \n",
    "For each word we calculate the P(word|spam) and P(word|nonspam) and calculate this which is a multiplication:\n",
    "\n",
    "$$\n",
    "\\approx\\frac{\\prod_i p(w_i\\ |\\ {\\rm spam})}{\\prod_i p(w_i\\ |\\ {\\rm spam})+\\prod_i p(w_i\\ |\\ {\\rm nonSpam})}\n",
    "$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bayes(txt):\n",
    "    \n",
    "    #finds the word count in unknown doc txt\n",
    "    wc=Counter(re.findall(\"[a-z0-9']+\", txt.lower()))\n",
    "    # keeping only 20 common words\n",
    "  \n",
    "    kw=[w for w,c in wc.most_common(15)]\n",
    "    \n",
    "    \n",
    "    t=1\n",
    "    k=1\n",
    "    for w in kw:\n",
    "        if w not in n_sp: n_sp[w]=0.1 # smoothing see appendix\n",
    "        if w not in n_nonsp: n_nonsp[w]=0.1\n",
    "        \n",
    "        t *= n_sp[w]/350\n",
    "        k *=n_nonsp[w]/350\n",
    "             \n",
    "\n",
    "    spamProbability= t/(t+k)\n",
    "    \n",
    "    if(spamProbability>=0.5):\n",
    "        print(\"it's probably spam\", spamProbability)\n",
    "    else:\n",
    "        print(\"it's probably not spam\", spamProbability)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the example below, correctly identifies all the emails that were \"not spam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100): # first 10 docs in the test set\n",
    "    Bayes(nospamtest[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correctly detects 9 out of 10 spam emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    Bayes(spamtest[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can tell that this very simple classifier works relatively well and there are ways for improving this which is a topic for later :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the example below (here's vs here & 's) for comparison of word tokenizer from nltk and find() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Here',\n",
       " \"'s\",\n",
       " 'a',\n",
       " 'test',\n",
       " 'of',\n",
       " 'word_tokenizer',\n",
       " 'and',\n",
       " 'find',\n",
       " 'function',\n",
       " '!']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "word_tokenize(\"Here's a test of word_tokenizer and find function!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"here's\", 'a', 'test', 'of', 'word_tokenizer', 'and', 'find', 'function']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"[a-z0-9'_]+\",\"Here's a test of word_tokenizer and find function!\".lower())\n",
    "#return all the words as a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counter is very useful as it keeps the counts in a dictionary for us. By adding the \"set\" we count all the occurances of a word in a document only once becasue we are itnerested in number of documents that has a word not the frequency of words per document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(set(re.findall(\"[a-z0-9'_]+\",spamtest[0].lower()))) # with set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter((re.findall(\"[a-z0-9'_]+\",spamtest[0].lower()))) # with no set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For words that are in the test document but haven't been in train set we need to do an adjustment. We set the number to 0.1 (as if the word occured in 0.1 of documents) to avoid multiplying by zero and losing data. This is callled \"smoothing\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
